{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorium 3: Augmentacje danych i balans klas (DermaMNIST)\n",
    "\n",
    "\n",
    "**Cele:**\n",
    "- porównać wpływ różnych **augmentacji** (geometria, intensywność, heavy) na wyniki,\n",
    "- zastosować metody **balansu klas**: wagi w `CrossEntropy`, `WeightedRandomSampler`,\n",
    "- przeprowadzić rzetelną **ewaluację** (Loss, Accuracy, Macro-Precision/Recall/F1, Confusion Matrix),\n",
    "- zebrać wyniki w tabeli i wyciągnąć wnioski.\n",
    "\n",
    "> **Dataset:** **DermaMNIST (MedMNIST)** – 7-klasowa klasyfikacja zmian skórnych, automatyczny download.\n",
    "\n",
    "> Uwaga: architektura CNN będzie prosta (dla 28×28×3); skupiamy się na augmentacjach i balansie klas.\n",
    "\n",
    "Augmentacja danych (ang. data augmentation) to zestaw technik sztucznego powiększania i urozmaicania zbioru treningowego poprzez modyfikacje istniejących danych. Istniejące próbki przekształca się tak, aby powstały ich różnorodne warianty, ale zachowujące tę samą etykietę (klasę). Przykłady dla obrazów:\n",
    "- obrót, odbicie lustrzane, przycięcie, skalowanie, przesunięcie,\n",
    "- zmiana jasności, kontrastu, koloru, dodanie szumu,\n",
    "- losowe maskowanie fragmentów obrazu (Cutout, Random Erasing).\n",
    "\n",
    "Stosowana jest ona w celu: zwiększenia rozmiaru zbioru treningowego – szczególnie ważne, gdy mamy mało przykładów, poprawy generalizacji – model uczy się odporniejszych cech (mniejszy overfitting), symulacji warunków rzeczywistych – różne oświetlenia, kąty patrzenia czy szumy w danych, wyrównania zbioru – generowanie dodatkowych przykładów dla klas, których jest mniej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Instalacja i importy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# !pip -q install medmnist torchmetrics tqdm scikit-learn pandas\n",
    "\n",
    "import os, random, math, json, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             confusion_matrix, ConfusionMatrixDisplay)\n",
    "\n",
    "import medmnist\n",
    "from medmnist import DermaMNIST, INFO\n",
    "\n",
    "SEED = 42\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "set_seed()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Pobranie danych (DermaMNIST, MedMNIST) + szybki podgląd i rozkład klas\n",
    "\n",
    "Skrypt automatycznie pobiera zbiór **DermaMNIST**. Poniższy kod sprawdza rozmiar danych i etykiety.\n",
    "Wyświetlone zostaje również 6 pierwszych próbek z tego zbioru.\n",
    "\n",
    "Zwróć uwagę na rozkład klas w wykorzystywanym zbiorze. Co można o nim powiedzieć?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opis: The DermaMNIST is based on the HAM10000, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. The dataset consists of 10,015 dermatoscopic images categorized as 7 different diseases, formulized as a multi-class classification task. We split the images into training, validation and test set with a ratio of 7:1:2. The source images of 3×600×450 are resized into 3×28×28.\n",
      "Liczba klas: 7 ; label map: {'0': 'actinic keratoses and intraepithelial carcinoma', '1': 'basal cell carcinoma', '2': 'benign keratosis-like lesions', '3': 'dermatofibroma', '4': 'melanoma', '5': 'melanocytic nevi', '6': 'vascular lesions'}\n",
      "Rozmiary: 7007 1003 2005\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozkład klas (train): {0: 228, 1: 359, 2: 769, 3: 80, 4: 779, 5: 4693, 6: 99}\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = './data/dermamnist'\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "info = INFO['dermamnist']\n",
    "NUM_CLASSES = len(info['label'])\n",
    "print('Opis:', info['description'])\n",
    "print('Liczba klas:', NUM_CLASSES, '; label map:', info['label'])\n",
    "\n",
    "# as_rgb=True -> 3 kanały\n",
    "train_raw = DermaMNIST(split='train', download=True, root=DATA_ROOT, as_rgb=True)\n",
    "val_raw   = DermaMNIST(split='val',   download=True, root=DATA_ROOT, as_rgb=True)\n",
    "test_raw  = DermaMNIST(split='test',  download=True, root=DATA_ROOT, as_rgb=True)\n",
    "print('Rozmiary:', len(train_raw), len(val_raw), len(test_raw))\n",
    "\n",
    "# podgląd kilku przykładów\n",
    "fig, axes = plt.subplots(1, 6, figsize=(10,2))\n",
    "for i in range(6):\n",
    "    img, y = train_raw[i]\n",
    "    axes[i].imshow(np.array(img))\n",
    "    axes[i].set_title(f'y={int(y.squeeze().item())}')\n",
    "    axes[i].axis('off')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Rozkład klas w train\n",
    "vals, cnts = np.unique(train_raw.labels.squeeze(), return_counts=True)\n",
    "print('Rozkład klas (train):', dict(zip(vals.tolist(), cnts.tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 1 – Dataset z przełączanymi augmentacjami i normalizacją\n",
    "\n",
    "W następnym fragmencie kodu chcemy przygotować różn warianty augmentacji danych, które będą wykorzystane do uczenia sieci i porównania wyników. W tym celu korzystamy z klasy `transforms` z biblioteki `torchvision`.\n",
    "\n",
    "1. Przygotuj najprostszą, bazową transformację danych. Zawiera ona przekształcenia, które będą obecne we wszystkich innych wariantach augmentacji.\n",
    "2. Do stworzenia pojedynczego wariantu wykorzystujemy funkcję `transforms.Compose`. Przyjmuje ona listę transformacji, które powinny być wykonane na wczytywanych danych.\n",
    "3. Podstawowy wariant powinien zawierać dwie transformacje: `transforms.ToTensor()` i `transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])`. Pierwsza z nich przekształca dane do typu tensora z PyTorch z odpowiednią kolejnością wymiarów. Dodatkowo dla obrazów typu `uint8` zmienia ich zakres do `[0.0, 1.0]`. Jeśli wejściem są wartości typu `float`, to zostaną one zachowane. Druga z kolei zmienia zakres danych do przedziału `[-1.0, 1.0]` (zakłądając, że wejście ma zakres `[0.0, 1.0]`). Jej arguemntami są paramtry normalizacji dla poszczególnych kanałów.\n",
    "4. Stwórz transformację, która przed bazowymi wykona dodatkowo odbicie w poziomie `transforms.RandomHorizontalFlip` i rotację obrazu `transforms.RandomRotation`. Argumentem pierwszej jest prawdopodobieństwo odbicia, a drugiej maksymalny kąt rotacji w stopniach.\n",
    "5. Stwórz transformację, która przed bazowymi wykona dodatkowo losową zmianę kolorów `transforms.ColorJitter`. Argumentami są maksymalne zmiany dla danej właściwości, np. `transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.05, hue=0.02)`.\n",
    "6. Czwarta transformacja powinna łączyć wszystkie wymienione wcześniej.\n",
    "7. Napisz również funkcję `make_datasets`, która będzie tworzyć DataSet i aplikować transformacje przekazane jako argument do danych treningowych. Zwracać powinna 3 zbiory danych: treningowy, walidacyjny i testowy. Dla danych walidacyjnych i testowych stosujemy tylko podstawowy wariant transformacji. Transformacja jest przekazywana jako dodatkowy arguement podczas inicjalizacji obiektu klasy `DermaMNIST`.\n",
    "8. Zdefiniuj `BATCH_SIZE`, `NUM_WORKERS` i `PIN_MEMORY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zadanie 1\n",
    "transform_1 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_2 = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(degrees=90),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_3 = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.05, hue=0.02),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_4 = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.05, hue=0.02),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(degrees=90),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "class DermaMNISTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        \n",
    "        if isinstance(img, np.ndarray):\n",
    "            img = Image.fromarray(img)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label = torch.tensor(label, dtype=torch.long).view(-1)\n",
    "        return img, label\n",
    "\n",
    "def create_dataset(dataset, transforms_list: list):\n",
    "    return [DermaMNISTDataset(dataset, tf) for tf in transforms_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 2 – Prosta sieć CNN 3-kanałowa + pętle trening/ewaluacja\n",
    "\n",
    "Zdefniujmy bardzo podobną sieć do wykorzystanej w poprzednim tygodniu. Poniżej znajduje się lista modyfikacji koniecznych do wykonania.\n",
    "\n",
    "1. Liczba kanałów wejściowych: 3.\n",
    "2. Liczba kanałów wyjściowych pierwszej warstwy konwolucyjnej: 16.\n",
    "3. Liczba kanałów wyjściowych drugiej warstwy konwolucyjnej: 32.\n",
    "4. Rozmiar obu konwolucji wynosi 3, a padding 1.\n",
    "5. Wyjście pierwszej warstwy w pełni połączonej ma rozmiar 128.\n",
    "6. Liczba klas (a więc i rozmiar wyjścia sieci) powinna być podawana jako argument.\n",
    "7. Dodajmy również warstwę dropout o prawdopodobieństwie równym 0.25 `nn.Dropout(p=0.25)` pomiędzy warstwami w pełni połączonymi. Warstwa ta losowo wyłącza część neuronów z zadanym prawdopodobieństwem. Warstwa taka może poprawić generalizację sieci i sprawić, że będzie się uczyć bardziej ogólnych cech.\n",
    "8. Zaimplementuj funkcję, która będzie wykonywać jedną epokę treningu (podobnie jak w poprzednim ćwiczeniu). Jej argumentami są trenowana sieć, DataLoader, optimalizator i obiekt funkcji straty. Nie musi ona liczyć dokładności, bo zaimplementujemy osobną funkcję do ewaluacji.\n",
    "9. Zaimplementuj funkcję wykonującą ewaluację sieci. Jak argumenty przyjmuje ona instancję sieci i DataLoader. Zapamiętaj w dwóch listach predykcje sieci i etykiety. Po przetworzeniu danych połącz je w jeden wektor za pomocą funkcji `np.concatenate` i oblicz metryki klasyfikacji takie jak: dokładność (accuracy), precyzja (precision), czułość (recall) i F1-score. W tym calu wykozystaj funkcje `accuracy_score`, `precision_score`, `recall_score` i `f1_score`.\n",
    "    - Dokładność jest odsetkiem poprawnie sklasyfikowanych próbek.\n",
    "    - Precyzja mówi ile z przewidzianych klasyfikacji danej klasy naprawdę należało do danej klasy. Dodajemy argument `average='macro`, który sprawia, że finalny wynik jest uśrednioną wartością dla wszystkich klas.\n",
    "    - Czułość mówi ile z prawdziwych przykładów danej klasy zostało poprawnie znalezionych przez model. W tym przypadku również chcemy uśrednić wyniki dla wszystkich klas.\n",
    "    - F1-score jest średnią harmoniczną z precyzji i czułości. Również chcemy go uśrednić dla wszystkich klas.\n",
    "Na wyjście sieci przekaż słownik zawierający obliczone metryki, wektor predykcji sieci oraz wektor rzeczywistych etykiet.\n",
    "\n",
    "Dlaczego w przypadku wykorzystywanego zbioru po prostu sprawdzanie dokładności może nie być dobrą metodą oceny jej skuteczności?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zadanie 2\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights)\n",
    "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "optim = torch.optim.Adam(params=model.parameters())\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "def run_epoch(model, dataloader, device, criterion, optimizer=None):\n",
    "    is_train = optimizer is not None\n",
    "    model.train(is_train)\n",
    "\n",
    "    lossTotal = 0.0\n",
    "    correctTotal = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device).long().view(-1)\n",
    "\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        lossTotal += loss.item() * x.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correctTotal += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    return lossTotal / total, correctTotal / total\n",
    "\n",
    "def eval_model(model, loader, device):\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for imgs, labels in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            preds_list.extend(pred.cpu().tolist())\n",
    "            labels_list.extend(labels.cpu().tolist())\n",
    "\n",
    "    acc = accuracy_score(labels_list, preds_list)\n",
    "    prec = precision_score(labels_list, preds_list, average='macro')\n",
    "    recall = recall_score(labels_list, preds_list, average='macro')\n",
    "    f1 = f1_score(labels_list, preds_list, average='macro')\n",
    "\n",
    "    return pd.DataFrame({\"acc\": [acc], \"prec\": [prec], \"recall\": [recall], \"f1\": [f1]})   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 3 – Przeprowadzenie treningu i sprawdzenie augmentacji\n",
    "\n",
    "Przeprowadź trening zdefiniowanej sieci dla każdego z wariantów augmentacji danych treningowych. Dla wielu przypadków testowych dobrze jest stworzyć strukturę zawierającą scenariusze testowe i uruchomić je w pętli.\n",
    "\n",
    "1. Dla każdego pzypadku stwórz 3 DataSety z odpowiednimi transformacjami (użyj przygotowanej wcześniej funkcji), a następnie dla każdego z nich DataLoader.\n",
    "2. Dla każdego przypadku stwórz instancję trenowanej sieci, optymalizator `torch.optim.Adam` i funkcję straty `nn.CrossEntropyLoss`.\n",
    "3. Stwórz puste listy do śledzenia postępu treningu: `train_losses`, `val_losses`, `train_accuracies`, `val_accuracies`, `val_f1_scores`, `epochs_list`.\n",
    "4. Dodatkowo chcemy zapamiętać model, który dla danego treningu osiągnął najlepszą wartość metryki F1-macro. Wymaga to zapamiętania najlepszej do tej pory widzianej metryki i poprawnej inicjalizacji.\n",
    "5. Napisz pętlę trenującą. W każdej iteracji chcemy:\n",
    "    - wykonać epokę treningu,\n",
    "    - obliczyć i zapamiętać metryki dla zbioru treningowego,\n",
    "    - obliczyć i zapamiętać metryki dla zbioru walidacyjnego,\n",
    "    - obliczyć i zapamiętać stratę dla zbioru treningowego,\n",
    "    - obliczyć i zapamiętać stratę dla zbioru walidacyjnego,\n",
    "    - sprawdzić czy jest to najlepszy do tej pory model,\n",
    "    - (opcjonalnie) wyświetlać wyniki po każdej epoce.\n",
    "6. Po zakończonym treningu wyświetl 2 wykresy:\n",
    "    - wykres staty dla zbioru treningowego i walidacyjnego,\n",
    "    - wykres wyznaczonych metryk dla zbioru treningowego i walidayjnego.\n",
    "7. Wykonaj ewaluację dla zbioru testowego. Wyświetl macierz pomyłek (podobnie jak w poprzednim ćwiczeniu).\n",
    "\n",
    "Dobrze jest zapamiętywać wyniki metryki zbioru testowego dla wszystkich testowanych przypadków i wyświetlić je na końcu w formie tabeli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trening dla transformacji 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.9036, Accuracy (train): 0.6792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████████████▋                                                                                                                                    | 1/10 [00:09<01:26,  9.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6674, Accuracy (train): 0.7562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████████████████████▍                                                                                                                     | 2/10 [00:18<01:13,  9.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5598, Accuracy (train): 0.7926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████████████████████████                                                                                                       | 3/10 [00:27<01:03,  9.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4644, Accuracy (train): 0.8253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████████████████████████████▊                                                                                        | 4/10 [00:36<00:54,  9.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3992, Accuracy (train): 0.8523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████████████████████████████▌                                                                         | 5/10 [00:45<00:45,  9.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3357, Accuracy (train): 0.8795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████████████████████████████████████████████▏                                                          | 6/10 [00:54<00:36,  9.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2805, Accuracy (train): 0.8957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉                                            | 7/10 [01:04<00:27,  9.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2290, Accuracy (train): 0.9192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                             | 8/10 [01:13<00:18,  9.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1929, Accuracy (train): 0.9318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎              | 9/10 [01:22<00:09,  9.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1670, Accuracy (train): 0.9418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [01:31<00:00,  9.15s/it]\n",
      "/home/patryk/Desktop/venv/ML_venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trening dla transformacji 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0127, Accuracy (train): 0.6535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████████████▋                                                                                                                                    | 1/10 [00:10<01:33, 10.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7981, Accuracy (train): 0.7217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████████████████████▍                                                                                                                     | 2/10 [00:20<01:23, 10.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7467, Accuracy (train): 0.7296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patryk/Desktop/venv/ML_venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      " 30%|████████████████████████████████████████████                                                                                                       | 3/10 [00:31<01:12, 10.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7231, Accuracy (train): 0.7383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████████████████████████████▊                                                                                        | 4/10 [00:41<01:02, 10.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6971, Accuracy (train): 0.7410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████████████████████████████▌                                                                         | 5/10 [00:51<00:51, 10.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6791, Accuracy (train): 0.7478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████████████████████████████████████████████▏                                                          | 6/10 [01:02<00:41, 10.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6620, Accuracy (train): 0.7564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉                                            | 7/10 [01:12<00:31, 10.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6513, Accuracy (train): 0.7560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                             | 8/10 [01:23<00:20, 10.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6341, Accuracy (train): 0.7618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎              | 9/10 [01:34<00:10, 10.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6404, Accuracy (train): 0.7664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [01:44<00:00, 10.47s/it]\n",
      "/home/patryk/Desktop/venv/ML_venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trening dla transformacji 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.9065, Accuracy (train): 0.6812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████████████▋                                                                                                                                    | 1/10 [00:14<02:10, 14.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6968, Accuracy (train): 0.7430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████████████████████▍                                                                                                                     | 2/10 [00:28<01:54, 14.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6113, Accuracy (train): 0.7796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████████████████████████                                                                                                       | 3/10 [00:42<01:40, 14.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5478, Accuracy (train): 0.7982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████████████████████████████▊                                                                                        | 4/10 [00:57<01:26, 14.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4578, Accuracy (train): 0.8325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████████████████████████████▌                                                                         | 5/10 [01:12<01:12, 14.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4054, Accuracy (train): 0.8504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████████████████████████████████████████████▏                                                          | 6/10 [01:26<00:57, 14.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3536, Accuracy (train): 0.8713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉                                            | 7/10 [01:41<00:43, 14.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3055, Accuracy (train): 0.8918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                             | 8/10 [01:55<00:28, 14.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2696, Accuracy (train): 0.9050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎              | 9/10 [02:10<00:14, 14.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2332, Accuracy (train): 0.9152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:24<00:00, 14.45s/it]\n",
      "/home/patryk/Desktop/venv/ML_venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trening dla transformacji 4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0335, Accuracy (train): 0.6422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patryk/Desktop/venv/ML_venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      " 10%|██████████████▋                                                                                                                                    | 1/10 [00:16<02:24, 16.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8177, Accuracy (train): 0.7111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████████████████████▍                                                                                                                     | 2/10 [00:32<02:08, 16.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7651, Accuracy (train): 0.7264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████████████████████████                                                                                                       | 3/10 [00:48<01:51, 15.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7320, Accuracy (train): 0.7364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patryk/Desktop/venv/ML_venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      " 40%|██████████████████████████████████████████████████████████▊                                                                                        | 4/10 [01:04<01:37, 16.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7194, Accuracy (train): 0.7391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patryk/Desktop/venv/ML_venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      " 50%|█████████████████████████████████████████████████████████████████████████▌                                                                         | 5/10 [01:22<01:23, 16.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7043, Accuracy (train): 0.7468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patryk/Desktop/venv/ML_venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      " 60%|████████████████████████████████████████████████████████████████████████████████████████▏                                                          | 6/10 [01:39<01:07, 16.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6984, Accuracy (train): 0.7471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patryk/Desktop/venv/ML_venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      " 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉                                            | 7/10 [01:55<00:49, 16.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6802, Accuracy (train): 0.7505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                             | 8/10 [02:11<00:33, 16.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6729, Accuracy (train): 0.7580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎              | 9/10 [02:28<00:16, 16.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6743, Accuracy (train): 0.7558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:44<00:00, 16.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        acc      prec    recall        f1    transform\n",
      "0  0.979592  0.961182  0.964495  0.961432  transform_1\n",
      "1  0.781504  0.616482  0.519603  0.550830  transform_2\n",
      "2  0.972742  0.952078  0.957053  0.954178  transform_3\n",
      "3  0.772513  0.592626  0.498199  0.524415  transform_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Zadanie 3\n",
    "transform_list = [transform_1, transform_2, transform_3, transform_4]\n",
    "\n",
    "NUM_CLASSES = len(np.unique(train_raw.labels))\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, tf in enumerate(transform_list):\n",
    "    print(f\"\\n--- Trening dla transformacji {i+1} ---\")\n",
    "    \n",
    "    dataset = DermaMNISTDataset(train_raw, tf)\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    model = resnet18(weights=ResNet18_Weights)\n",
    "    model.fc = torch.nn.Linear(model.fc.in_features, NUM_CLASSES)  \n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "        loss, acc_train = run_epoch(model, loader, device, criterion, optimizer)\n",
    "        print(f\"Loss: {loss:.4f}, Accuracy (train): {acc_train:.4f}\")\n",
    "        \n",
    "        eval_loader = DataLoader(dataset, batch_size=32)\n",
    "        df_metrics = eval_model(model, eval_loader, device)\n",
    "        df_metrics[\"transform\"] = f\"transform_{i+1}\"\n",
    "    \n",
    "    results.append(df_metrics)\n",
    "\n",
    "final_results = pd.concat(results, ignore_index=True)\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 4 – Balans klas: wagi, WeightedRandomSampler\n",
    "\n",
    "Wykorzystamy dodatkowo dwie metody radzenia sobie ze zbiorami danych i niezbalansowanych klasach.\n",
    "Pierwszą są wagi klas (class weights). Działa to w ten sposób, że jeśli jakaś klasa jest rzadka, to jej błędy mają być bardziej kosztowne. Dodając większe wagi dla rzadkich klas w funkcji straty, uczymy model, żeby przykładał do nich większą wagę.\n",
    "\n",
    "Drugą metodą jest zmiana doboru próbek (WeightedRandomSampler). W tym przypadku rzadkie klasy są częściej losowane do batcha, a więc model widzi ich proporcjonalnie więcej. Również w tym przypadku tworzymy wagi dla każdej klasy.\n",
    "\n",
    "1. Zaimplementuj funkcję, która będzie obliczać wagi dla klas. Jej argumentem jest lista rzeczywistych etykiet zbioru treningowego (metoda `.labels` dla zbioru treningowego).\n",
    "2. Wewnątrz funkcji sprawdzamy ile jest wystąpień dla każdej etykiety (funkcja `np.unique`). Jej pierwszym argumentem są etykiety. Przekazujemy jej również argument `return_counts=True`, który powoduje, że funkcja zwraca również liczbę wystąpień.\n",
    "3. Następnie dzielimy otrzymane wystąpienia przez liczbę próbek. W ten sposób obliczamy częstość występowania danej klasy.\n",
    "4. Chcemy, aby waga była odwrotnie proporcjonalna do częstości jej występowania, a więc obliczamy odwrotność poprzedniej wartości. Warto dodać zabezpieczenie przed zbyt małą wartością mianownika `np.maximum(freq, 1e-8)`.\n",
    "5. Normalizujemy wartości obliczonych wag tak, żeby ich suma wy|nosiła liczbę możliwych etykiet.\n",
    "6. Zwracamy tensor wag `torch.tensor` z danymi typu `torch.float32`.\n",
    "7. Wyświetl obliczone wagi.\n",
    "8. Na podstawie wag zwróconych przez zaimplementowaną funkcję stworzymy sampler do danych treningowych. W tym celu należy obliczyć wagę dla każdej próbki ze zbioru treningowego. Bierzemy więc listę etykiet próbek i sprawdzamy jakie prawdopodobieństwo odpowiada każdej z nich (a więc robimy Look-Up Table).\n",
    "9. Obliczone nagi dla każdej próbki przezkazujemy do klasy `WeightedRandomSampler` jako argument `weights`. Oprócz tego konieczne jst przekazanie argumentu `num_samples`, czyli liczba próbek, i podanie argumentu `replacement` jako `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zadanie 4\n",
    "\n",
    "classes, counts = np.unique(train_raw.labels, return_counts=True)\n",
    "freq = counts / len(train_raw.labels)\n",
    "inv_freq = 1 / np.maximum(freq, 1e-8)\n",
    "weights = torch.tensor(inv_freq * len(classes) / np.sum(inv_freq), dtype=torch.float32)\n",
    "\n",
    "sample_weights = torch.tensor([weights[label] for label in classes])\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(train_raw.labels),\n",
    "    replacement=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 5 – Przeprowadzenie treningu i sprawdzenie wpływu testowanych metod na wynik\n",
    "\n",
    "Przeprowadzamy trening podobno do zadania 3, ale tym razem zamiast metod augumentacji sprawdzamy wpływ wag strat i częstości doboru próbek.\n",
    "\n",
    "1. Aby sprawdzić wpływ wag, do funkcji straty należy przekazać ich wektor `nn.CrossEntropyLoss(weight=weights_ce.to(device))`.\n",
    "2. Aby sprawdzić wpływ samplera, należy go przekazać do DataLoadera jako argument `sampler`.\n",
    "3. Sprawdź wpływ zaimplementowanych metod na wyniki treningu przeprowadzając eksperymenty w podobny sposób jak w zadaniu 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████████████▋                                                                                                                                    | 1/10 [00:12<01:52, 12.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] Train Loss: 0.0844 | Train Acc: 0.9710 || Val Loss: 5.0650 | Val Acc: 0.5374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████████████████████▍                                                                                                                     | 2/10 [00:24<01:39, 12.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/10] Train Loss: 0.0266 | Train Acc: 0.9910 || Val Loss: 4.7720 | Val Acc: 0.6371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████████████████████████                                                                                                       | 3/10 [00:37<01:27, 12.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/10] Train Loss: 0.0138 | Train Acc: 0.9946 || Val Loss: 5.5019 | Val Acc: 0.6042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████████████████████████████▊                                                                                        | 4/10 [00:50<01:16, 12.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/10] Train Loss: 0.0190 | Train Acc: 0.9934 || Val Loss: 6.7552 | Val Acc: 0.6102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████████████████████████████▌                                                                         | 5/10 [01:02<01:03, 12.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/10] Train Loss: 0.0052 | Train Acc: 0.9984 || Val Loss: 6.5982 | Val Acc: 0.5862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████████████████████████████████████████████▏                                                          | 6/10 [01:15<00:50, 12.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/10] Train Loss: 0.0137 | Train Acc: 0.9961 || Val Loss: 6.5573 | Val Acc: 0.5374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉                                            | 7/10 [01:28<00:37, 12.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/10] Train Loss: 0.0104 | Train Acc: 0.9957 || Val Loss: 4.4528 | Val Acc: 0.5244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                             | 8/10 [01:40<00:25, 12.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/10] Train Loss: 0.0435 | Train Acc: 0.9899 || Val Loss: 8.1268 | Val Acc: 0.6241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎              | 9/10 [01:53<00:12, 12.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/10] Train Loss: 0.0091 | Train Acc: 0.9977 || Val Loss: 6.9080 | Val Acc: 0.6331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:06<00:00, 12.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/10] Train Loss: 0.0074 | Train Acc: 0.9980 || Val Loss: 5.5462 | Val Acc: 0.5972\n",
      "\n",
      "Ewaluacja na zbiorze testowym:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        acc      prec    recall        f1\n",
      "0  0.590524  0.159973  0.226261  0.179144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patryk/Desktop/venv/ML_venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# Zadanie 5\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "model = model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weights.to(device))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "train_dataloader = DataLoader(DermaMNISTDataset(train_raw, transform=transform), batch_size=8, sampler=sampler)\n",
    "val_dataloader = DataLoader(DermaMNISTDataset(val_raw, transform=transform), batch_size=8)\n",
    "test_dataloader = DataLoader(DermaMNISTDataset(test_raw, transform=transform), batch_size=8)\n",
    "\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "    train_loss, train_acc = run_epoch(model, train_dataloader, device, loss_fn, optimizer)\n",
    "    val_loss, val_acc = run_epoch(model, val_dataloader, device, loss_fn)\n",
    "\n",
    "    print(f\"[{epoch+1}/{NUM_EPOCHS}] \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} \"\n",
    "          f\"|| Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "print(\"\\nEwaluacja na zbiorze testowym:\")\n",
    "test_metrics = eval_model(model, test_dataloader, device)\n",
    "print(test_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 6 – Eksperymenty własne\n",
    "\n",
    "Spróbuj połączyć poznane metody tak, aby uzysać jak największy wskaźnik F1-macro. Możesz dodatkowo użyć innych metod augmentacji danych, takich jak: `transforms.RandomVerticalFlip`, `transforms.RandomAffine`, `transforms.RandomResizedCrop`, `transforms.GaussianBlur`, `transforms.RandomApply`, `transforms.RandomAdjustSharpness`, `transforms.RandomGrayscale`, `transforms.RandomErasing`, `transforms.Lambda` (może zostać użyte do uzyskania szumu Gaussowskiego: `transforms.Lambda(lambda x: torch.clamp(x + 0.03*torch.randn_like(x), -1, 1))`, w tej formie używana już po normalizacji zakresu), `transforms.TrivialAugmentWide`, `transforms.RandAugment`.\n",
    "\n",
    "Pochwal się uzyskanym wynikiem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██▏                                         | 1/20 [00:13<04:24, 13.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/20] Train Loss: 0.1731 | Train Acc: 0.9070 || Val Loss: 2.5846 | Val Acc: 0.6720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████▍                                       | 2/20 [00:27<04:10, 13.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/20] Train Loss: 0.1670 | Train Acc: 0.9060 || Val Loss: 2.3102 | Val Acc: 0.7039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|██████▌                                     | 3/20 [00:41<03:52, 13.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/20] Train Loss: 0.1773 | Train Acc: 0.9021 || Val Loss: 2.2563 | Val Acc: 0.6780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████▊                                   | 4/20 [00:55<03:40, 13.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/20] Train Loss: 0.2058 | Train Acc: 0.9070 || Val Loss: 2.8360 | Val Acc: 0.6700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████                                 | 5/20 [01:08<03:25, 13.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/20] Train Loss: 0.2308 | Train Acc: 0.8780 || Val Loss: 2.6534 | Val Acc: 0.6560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████▏                              | 6/20 [01:22<03:12, 13.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/20] Train Loss: 0.2597 | Train Acc: 0.8691 || Val Loss: 2.9187 | Val Acc: 0.6879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███████████████▍                            | 7/20 [01:37<03:02, 14.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/20] Train Loss: 0.2054 | Train Acc: 0.8895 || Val Loss: 4.4807 | Val Acc: 0.6660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████▌                          | 8/20 [01:51<02:50, 14.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/20] Train Loss: 0.1493 | Train Acc: 0.9159 || Val Loss: 4.2729 | Val Acc: 0.6780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|███████████████████▊                        | 9/20 [02:05<02:35, 14.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/20] Train Loss: 0.1468 | Train Acc: 0.9187 || Val Loss: 3.2117 | Val Acc: 0.6790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████▌                     | 10/20 [02:19<02:21, 14.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/20] Train Loss: 0.1575 | Train Acc: 0.9087 || Val Loss: 3.1361 | Val Acc: 0.6640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|███████████████████████▋                   | 11/20 [02:34<02:07, 14.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/20] Train Loss: 0.1840 | Train Acc: 0.9025 || Val Loss: 5.0314 | Val Acc: 0.6760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████▊                 | 12/20 [02:48<01:53, 14.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/20] Train Loss: 0.1689 | Train Acc: 0.9149 || Val Loss: 3.0892 | Val Acc: 0.6640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|███████████████████████████▉               | 13/20 [03:02<01:38, 14.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/20] Train Loss: 0.2189 | Train Acc: 0.8940 || Val Loss: 2.8232 | Val Acc: 0.6690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████             | 14/20 [03:16<01:25, 14.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/20] Train Loss: 0.1336 | Train Acc: 0.9169 || Val Loss: 2.9229 | Val Acc: 0.6889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████████████████████████████████▎          | 15/20 [03:30<01:11, 14.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15/20] Train Loss: 0.1563 | Train Acc: 0.9244 || Val Loss: 3.0560 | Val Acc: 0.6889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████▍        | 16/20 [03:45<00:57, 14.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16/20] Train Loss: 0.1594 | Train Acc: 0.9157 || Val Loss: 4.2685 | Val Acc: 0.6680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████████████████████████████████▌      | 17/20 [04:00<00:43, 14.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17/20] Train Loss: 0.1199 | Train Acc: 0.9326 || Val Loss: 3.8305 | Val Acc: 0.7039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████▋    | 18/20 [04:14<00:28, 14.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18/20] Train Loss: 0.1306 | Train Acc: 0.9295 || Val Loss: 3.4614 | Val Acc: 0.7069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|████████████████████████████████████████▊  | 19/20 [04:29<00:14, 14.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19/20] Train Loss: 0.1248 | Train Acc: 0.9328 || Val Loss: 2.7146 | Val Acc: 0.7218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 20/20 [04:43<00:00, 14.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/20] Train Loss: 0.1605 | Train Acc: 0.9268 || Val Loss: 2.5724 | Val Acc: 0.6929\n",
      "\n",
      "Ewaluacja na zbiorze testowym:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        acc      prec    recall       f1\n",
      "0  0.689277  0.475329  0.517061  0.49346\n"
     ]
    }
   ],
   "source": [
    "# --- Wagi klas ---\n",
    "train_raw = DermaMNIST(split='train', download=True, as_rgb=True)\n",
    "val_raw   = DermaMNIST(split='val',   download=True, as_rgb=True)\n",
    "test_raw  = DermaMNIST(split='test',  download=True, as_rgb=True)\n",
    "\n",
    "count = np.unique(train_raw.labels, return_counts=True)[1]\n",
    "weights = torch.tensor((1 / count) * sum(count), dtype=torch.float32)\n",
    "weights = weights / weights.sum()  # normalizacja\n",
    "weights = weights.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# model = resnet18(weights=ResNet18_Weights)\n",
    "# model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weights)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- DataLoadery ---\n",
    "train_dataloader = DataLoader(DermaMNISTDataset(train_raw, transform=transform),\n",
    "                              batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(DermaMNISTDataset(val_raw, transform=transform),\n",
    "                            batch_size=8, shuffle=False)\n",
    "test_dataloader = DataLoader(DermaMNISTDataset(test_raw, transform=transform),\n",
    "                             batch_size=8, shuffle=False)\n",
    "\n",
    "# --- Pętla treningowa ---\n",
    "for epoch in tqdm(range(20)):\n",
    "    train_loss, train_acc = run_epoch(model, train_dataloader, device, loss_fn, optimizer)\n",
    "    val_loss, val_acc = run_epoch(model, val_dataloader, device, loss_fn)\n",
    "\n",
    "    print(f\"[{epoch+1}/{20}] \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} \"\n",
    "          f\"|| Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# --- Ewaluacja na zbiorze testowym ---\n",
    "print(\"\\nEwaluacja na zbiorze testowym:\")\n",
    "test_metrics = eval_model(model, test_dataloader, device)\n",
    "print(test_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podsumowanie – Wnioski\n",
    "\n",
    "- Która strategia **augmentacji** dała najwyższą Macro-F1?\n",
    "- Czy **wagi** w CE lub **sampler** poprawiły Recall klas rzadkich?\n",
    "- Jakie klasy najczęściej się mylą (macierz pomyłek)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML_venv)",
   "language": "python",
   "name": "ml_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
